{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 5: PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTHOR**: CAT LUONG (luongcn) <br>\n",
    "**DATE**: 03/31/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import and Spark Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import max, min, month, col, mean, desc, stddev\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Sirius:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x221b95a3dc0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intialize spark session\n",
    "spark=SparkSession.builder.appName(\"Practise\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS FOR EACH TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/2010/01052099999.csv'), WindowsPath('data/2010/99407099999.csv'), WindowsPath('data/2011/01008099999.csv'), WindowsPath('data/2011/01046099999.csv'), WindowsPath('data/2012/01023099999.csv'), WindowsPath('data/2012/01044099999.csv'), WindowsPath('data/2013/01001499999.csv'), WindowsPath('data/2013/01008099999.csv'), WindowsPath('data/2014/01008099999.csv'), WindowsPath('data/2014/01023099999.csv'), WindowsPath('data/2015/01008099999.csv'), WindowsPath('data/2015/01025099999.csv'), WindowsPath('data/2016/01008099999.csv'), WindowsPath('data/2016/01023199999.csv'), WindowsPath('data/2017/01008099999.csv'), WindowsPath('data/2017/01023099999.csv'), WindowsPath('data/2018/01008099999.csv'), WindowsPath('data/2018/01025099999.csv'), WindowsPath('data/2019/01008099999.csv'), WindowsPath('data/2019/01023099999.csv'), WindowsPath('data/2020/01008099999.csv'), WindowsPath('data/2020/01023099999.csv'), WindowsPath('data/2021/01062099999.csv'), WindowsPath('data/2021/01065099999.csv'), WindowsPath('data/2022/01241099999.csv'), WindowsPath('data/2022/02095099999.csv')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"data\")\n",
    "files  = [f for f in data_path.rglob(\"*\") if f.is_file() and f.parent.stem != '.ipynb_checkpoints']\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_temp(file_1: Path, file_2: Path, column_name: str = 'MAX') -> Tuple[str, str]:   \n",
    "    \"\"\"\n",
    "    Task 1: Find the hottest day (column MAX) for each year, and provide the corresponding station code,\n",
    "    station name and the date (columns STATION, NAME, DATE).\n",
    "    \"\"\"\n",
    "    year = file_1.parent.stem\n",
    "    df_pyspark_1 = spark.read.option(\"header\",\"true\").csv(str(file_1))\n",
    "    df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(str(file_2))\n",
    "\n",
    "    max_temp_df_1 = df_pyspark_1.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "    max_temp_df_2 = df_pyspark_2.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "\n",
    "    combined_max_temp_df = max_temp_df_1.unionByName(max_temp_df_2)\n",
    "    combined_max_temp_df = combined_max_temp_df.filter(col(column_name) != '9999.9')\n",
    "\n",
    "    # Initilize window specification\n",
    "    w = Window.partitionBy()\n",
    "\n",
    "    # Apply the max function using the window specification\n",
    "    combined_max_temp_df = combined_max_temp_df.withColumn('max_temp', max(col(column_name).cast('float')).over(w))\n",
    "\n",
    "    # Filter the DataFrame to get rows where TEMP equals max_temp\n",
    "    filtered_df = combined_max_temp_df.filter(combined_max_temp_df[column_name] == combined_max_temp_df['max_temp'])\n",
    "\n",
    "    # Show the filtered DataFrame\n",
    "    filtered_df.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "\n",
    "    filtered_df = filtered_df.collect()\n",
    "    \n",
    "    max_val = f\"MAX: {filtered_df[0][0]}, STATION: {filtered_df[0][1]}, NAME: {filtered_df[0][2]}, DATE: {filtered_df[0][3]}\"\n",
    "\n",
    "    return year, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_temp_jan(file_1: Path, file_2: Path, column_name: str = 'MIN') -> Tuple[str, str, float]: \n",
    "    \"\"\"\n",
    "    Task 2: Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) ,\n",
    "    and provide the corresponding station code, station name and the date (columns STATION,\n",
    "    NAME, DATE).\n",
    "    \"\"\"\n",
    "    year = file_1.parent.stem\n",
    "    df_pyspark_1 = spark.read.option(\"header\",\"true\").csv(str(file_1))\n",
    "    df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(str(file_2))\n",
    "\n",
    "    max_temp_df_1 = df_pyspark_1.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "    max_temp_df_2 = df_pyspark_2.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "\n",
    "    combined_min_temp_df = max_temp_df_1.unionByName(max_temp_df_2)\n",
    "    combined_min_temp_df = combined_min_temp_df.filter(col(column_name) != '9999.9')\n",
    "\n",
    "    # Get the month of the date column\n",
    "    combined_min_temp_df = combined_min_temp_df.withColumn('month', month(combined_min_temp_df['DATE']))\n",
    "\n",
    "    # Filter the DataFrame to only include rows where the month is 3\n",
    "    combined_min_temp_df = combined_min_temp_df.filter(combined_min_temp_df['month'] == 1)\n",
    "\n",
    "    # Initilize window specification\n",
    "    w = Window.partitionBy()\n",
    "\n",
    "    # Apply the max function using the window specification\n",
    "    combined_min_temp_df = combined_min_temp_df.withColumn('min_temp', min(col(column_name).cast('float')).over(w))\n",
    "\n",
    "    # Filter the DataFrame to get rows where TEMP equals max_temp\n",
    "    filtered_df = combined_min_temp_df.filter(combined_min_temp_df[column_name] == combined_min_temp_df['min_temp'])\n",
    "\n",
    "    # Show the filtered DataFrame\n",
    "    filtered_df.select([column_name, 'STATION', 'NAME', 'DATE'])\n",
    "\n",
    "    filtered_df = filtered_df.collect()\n",
    "    \n",
    "    min_str = f\"MIN: {filtered_df[0][0]}, STATION: {filtered_df[0][1]}, NAME: {filtered_df[0][2]}, DATE: {filtered_df[0][3]}\"\n",
    "    min_temp = float(filtered_df[0][0])\n",
    "    \n",
    "    return year, min_str, min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_precipitation(file_1: Path, file_2: Path, column_name: str = 'PRCP') -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Task 3: Maximum and Minimum precipitation (column PRCP ) for the year 2015, and provide the\n",
    "    corresponding station code, station name and the date (columns STATION, NAME, DATE).\n",
    "    \"\"\"\n",
    "    df_pyspark_1 = spark.read.option(\"header\",\"true\").csv(str(file_1))\n",
    "    df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(str(file_2))\n",
    "\n",
    "    df_1 = df_pyspark_1.select(['PRCP', 'STATION', 'NAME', 'DATE'])\n",
    "    df_2 = df_pyspark_2.select(['PRCP', 'STATION', 'NAME', 'DATE'])\n",
    "\n",
    "    # Initilize window specification\n",
    "    w = Window.partitionBy()\n",
    "\n",
    "    combined_prcp_df = df_1.unionByName(df_2)\n",
    "    combined_prcp_df = combined_prcp_df.filter(col(column_name) != '99.99')\n",
    "\n",
    "    combined_prcp_df_max = combined_prcp_df.withColumn('max_prcp', max(col(column_name).cast('float')).over(w))\n",
    "    max_df = combined_prcp_df_max.filter(combined_prcp_df_max[column_name] == combined_prcp_df_max['max_prcp']).collect()\n",
    "\n",
    "    combined_prcp_df_min = combined_prcp_df.withColumn('min_prcp', min(col(column_name).cast('float')).over(w))\n",
    "    min_df = combined_prcp_df_min.filter(combined_prcp_df_min[column_name] == combined_prcp_df_min['min_prcp']).collect()\n",
    "\n",
    "    # Get max and min values of precipitation\n",
    "    max_val = f\"PRCP: {max_df[0][0]}, STATION: {max_df[0][1]}, NAME: {max_df[0][2]}, DATE: {max_df[0][3]}\"\n",
    "    min_val = f\"PRCP: {min_df[0][0]}, STATION: {min_df[0][1]}, NAME: {min_df[0][2]}, DATE: {min_df[0][3]}\"\n",
    "\n",
    "    return max_val, min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_values(file_1: Path, file_2: Path, column_name: str = 'GUST') -> float: \n",
    "    \"\"\"\n",
    "    Task 4:  Count percentage missing values for wind gust (column GUST) for the year 2019. \n",
    "    \"\"\"\n",
    "    df_pyspark_1 = spark.read.option(\"header\",\"true\").csv(str(file_1))\n",
    "    df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(str(file_2))\n",
    "\n",
    "    rows_1 = df_pyspark_1.count()\n",
    "    rows_2 = df_pyspark_2.count()\n",
    "\n",
    "    missing_column_count_1 = df_pyspark_1.filter(col(column_name) == '999.9').count()\n",
    "    missing_column_count_2 = df_pyspark_2.filter(col(column_name) == '999.9').count()\n",
    "\n",
    "    return (missing_column_count_2 + missing_column_count_1) / (rows_1 + rows_2) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMM_std_temp(file_1: Path, file_2: Path, column_name: str = 'TEMP') -> dict[str, list[str, str, str, str]]:\n",
    "    \"\"\"\n",
    "    Task 5: Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for\n",
    "    each month for the year 2020.\n",
    "    \"\"\"\n",
    "    df_pyspark_1 = spark.read.option(\"header\",\"true\").csv(str(file_1))\n",
    "    df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(str(file_2))\n",
    "\n",
    "    temp_1_df = df_pyspark_1.filter(col(column_name) != '9999.9')\n",
    "    temp_2_df = df_pyspark_2.filter(col(column_name) != '9999.9')\n",
    "    \n",
    "    \n",
    "    temp_1_df = temp_1_df.withColumn(column_name, col(column_name).cast('float'))\n",
    "    temp_2_df = temp_2_df.withColumn(column_name, col(column_name).cast('float'))\n",
    "\n",
    "    combined_temp_df = temp_1_df.unionByName(temp_2_df)\n",
    "    MMM_std = {}\n",
    "\n",
    "    for m in range(1, 13, 1):\n",
    "        temp_df = combined_temp_df.withColumn('month', month(combined_temp_df['DATE']))\n",
    "        temp_df = temp_df.filter(temp_df['month'] == m)\n",
    "        # Calculate the mean\n",
    "        avg = temp_df.agg(mean(column_name)).collect()[0][0]\n",
    "\n",
    "        # Calculate the median\n",
    "        median = temp_df.approxQuantile(column_name, [0.5], 0)[0]\n",
    "\n",
    "        # Calculate the mode\n",
    "        mode = temp_df.groupBy(column_name).count().orderBy(desc(\"count\")).first()[0]\n",
    "\n",
    "        # Calculate the standard deviation\n",
    "        std = temp_df.agg(stddev(column_name)).collect()[0][0]\n",
    "\n",
    "        MMM_std[str(calendar.month_name[m])] = f\"Mean: {avg}, Median: {median}, Mode: {mode}, Standard Deviation: {std}\"\n",
    "\n",
    "    return MMM_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 and Task 2 results\n",
    "task1_results = {}\n",
    "task2_results = \"\"\n",
    "MIN_VAL = 1e9\n",
    "\n",
    "for i in range(0, len(files), 2): \n",
    "    year, max_temp_str = get_max_temp(files[i], files[i+1])\n",
    "    year, min_temp_str, min_temp = get_min_temp_jan(files[i], files[i+1])\n",
    "    task1_results[year] = max_temp_str\n",
    "    if min_temp < MIN_VAL:\n",
    "        task2_results = f\"YEAR: {year}, {min_temp_str}\"\n",
    "        MIN_VAL = min_temp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2010': 'MAX:   74.8, STATION: 99407099999, NAME: DESTRUCTION IS. WA, WA US, DATE: 2010-08-15',\n",
       " '2011': 'MAX:   87.8, STATION: 01046099999, NAME: SORKJOSEN, NO, DATE: 2011-07-09',\n",
       " '2012': 'MAX:   72.0, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2012-07-05',\n",
       " '2013': 'MAX:   80.6, STATION: 01001499999, NAME: SORSTOKKEN, NO, DATE: 2013-08-02',\n",
       " '2014': 'MAX:   89.6, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2014-07-10',\n",
       " '2015': 'MAX:   71.6, STATION: 01025099999, NAME: TROMSO, NO, DATE: 2015-07-30',\n",
       " '2016': 'MAX:   77.0, STATION: 01023199999, NAME: DRAUGEN, NO, DATE: 2016-07-21',\n",
       " '2017': 'MAX:   78.6, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2017-06-09',\n",
       " '2018': 'MAX:   84.2, STATION: 01025099999, NAME: TROMSO, NO, DATE: 2018-07-29',\n",
       " '2019': 'MAX:   78.8, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2019-07-21',\n",
       " '2020': 'MAX:   79.9, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2020-06-22',\n",
       " '2021': 'MAX:   88.3, STATION: 01065099999, NAME: KARASJOK, NO, DATE: 2021-07-05',\n",
       " '2022': 'MAX:   85.5, STATION: 02095099999, NAME: PAJALA, SW, DATE: 2022-07-01'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YEAR: 2017, MIN:  -28.3, STATION: 01023099999, NAME: BARDUFOSS, NO, DATE: 2017-01-05'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRCP:  2.11, STATION: 01025099999, NAME: TROMSO, NO, DATE: 2015-11-02',\n",
       " 'PRCP:  0.00, STATION: 01008099999, NAME: LONGYEAR, SV, DATE: 2015-01-01']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3 Results\n",
    "path_2015 = data_path / Path('2015')\n",
    "files_2015 = [f for f in path_2015.glob('*') if f.is_file()]\n",
    "\n",
    "task3_results = list(min_max_precipitation(files_2015[0], files_2015[1]))\n",
    "task3_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Missing value percentage of GUST for 2019 is 82.87671232876713%'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 4 Results\n",
    "path_2019 = data_path / Path('2019')\n",
    "files_2019 = [f for f in path_2019.glob('*') if f.is_file()]\n",
    "task4_results = f\"Missing value percentage of GUST for 2019 is {count_missing_values(files_2019[0], files_2019[1])}%\"\n",
    "task4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2020 = data_path / Path('2020')\n",
    "files_2020 = [f for f in path_2020.glob('*') if f.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'January': 'Mean: 15.896774190928667, Median: 14.899999618530273, Mode: 5.699999809265137, Standard Deviation: 12.805172732458736',\n",
       " 'February': 'Mean: 13.358620645671055, Median: 15.300000190734863, Mode: 2.799999952316284, Standard Deviation: 13.091808562182692',\n",
       " 'March': 'Mean: 14.65322592914585, Median: 18.600000381469727, Mode: 9.199999809265137, Standard Deviation: 15.784789655679134',\n",
       " 'April': 'Mean: 23.329999959468843, Median: 26.0, Mode: 34.099998474121094, Standard Deviation: 13.022097154673123',\n",
       " 'May': 'Mean: 36.21935490638979, Median: 36.0, Mode: 37.0, Standard Deviation: 8.077246838395743',\n",
       " 'June': 'Mean: 47.429999923706056, Median: 46.0, Mode: 36.70000076293945, Standard Deviation: 8.877190496403378',\n",
       " 'July': 'Mean: 52.8870968972483, Median: 51.400001525878906, Mode: 49.29999923706055, Standard Deviation: 6.66378725061258',\n",
       " 'August': 'Mean: 49.2870969464702, Median: 48.70000076293945, Mode: 44.70000076293945, Standard Deviation: 6.548594719618069',\n",
       " 'September': 'Mean: 41.8450000445048, Median: 42.5, Mode: 31.799999237060547, Standard Deviation: 5.887661107090601',\n",
       " 'October': 'Mean: 31.529032414959325, Median: 30.700000762939453, Mode: 32.20000076293945, Standard Deviation: 9.609052989565193',\n",
       " 'November': 'Mean: 29.246666606267294, Median: 29.799999237060547, Mode: 36.29999923706055, Standard Deviation: 8.143448333355236',\n",
       " 'December': 'Mean: 19.95483873736474, Median: 20.200000762939453, Mode: 10.199999809265137, Standard Deviation: 8.854464180955505'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 5 Results\n",
    "task5_results = MMM_std_temp(files_2020[0], files_2020[1])\n",
    "task5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT the results to results.txt\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Task 1: \\n\")\n",
    "    for year, val in task1_results.items():\n",
    "        f.write(f\"{year}: {val}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Task 2: \\n\")\n",
    "    f.write(task2_results + \"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Task 3: \\n\")\n",
    "    for val in task3_results: \n",
    "        f.write(val + \"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Task 4: \\n\")\n",
    "    f.write(task4_results + \"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Task 5: \\n\")\n",
    "    for month, val in task5_results.items():\n",
    "        f.write(f\"{month}: {val} \\n\")\n",
    "    f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
